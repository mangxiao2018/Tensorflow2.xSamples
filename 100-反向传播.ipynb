{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 反向传播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.constant 创建一个张量\n",
    "# tf.constant(\n",
    "#     value, dtype=None, shape=None, name='Const'\n",
    "# )\n",
    "# value是必选项，dtype、shape、name是可选项\n",
    "# value可以是一个标量，也可以是python的一个列表\n",
    "# dtype：数据类型，可以是float32,也可以是float64\n",
    "# shape: 表示张量的“形状”，即维数以及每一维的大小,\n",
    "# 1、如果第一个参数value是数字时，张量的所有元素都会用该数字填充\n",
    "# 例如：tf.constant(-1, shape=[2, 3])\n",
    "# 结果:  [[-1 -1 -1]\n",
    "#        [-1 -1 -1]]\n",
    "# 2、而当第一个参数value是一个列表时，注意列表的长度必须小于等于第三个参数shape的大小（即各维大小的乘积）\n",
    "# 例如：tensor=tf.constant([1, 2, 3, 4, 5, 6, 7], shape=[2, 3])\n",
    "# 列表大小为7，而shape大小为2*3=6，无法正确填充，所以发生了错误。\n",
    "# 3、而如果列表大小小于shape大小，则会用列表的最后一项元素填充剩余的张量元素\n",
    "# 例如：tensor=tf.constant([1, 2], shape=[1, 4, 3])\n",
    "# 结果： [[[1 2 2]\n",
    "#         [2 2 2]\n",
    "#         [2 2 2]\n",
    "#         [2 2 2]]]\n",
    "# 第四个参数name可以是任何内容，主要是字符串就行\n",
    "# 【注意】v2.2.0没有了第五个参数verify_shape默认为False，如果修改为True的话表示检查value的形状与shape是否相符，如果不符会报错。\n",
    "# --------------------------------------------------------------------------------------\n",
    "# tf.Variable(\n",
    "#     initial_value=None, trainable=None, validate_shape=True, caching_device=None,\n",
    "#     name=None, variable_def=None, dtype=None, import_scope=None, constraint=None,\n",
    "#     synchronization=tf.VariableSynchronization.AUTO,\n",
    "#     aggregation=tf.compat.v1.VariableAggregation.NONE, shape=None\n",
    "# )\n",
    "#\n",
    "# Vatiable是tensorflow的变量节点，通过Variable方法创建，并且需要传递初始值。在使用前需要通过tensorflow的初始化方法进行初始化\n",
    "#\n",
    "# W = tf.Variable(\n",
    "#                 initial_value=tf.zeros([9, 5]),  \n",
    "#                         # 初始值，必填，张量或可以转换为张量的Python对象。初始值必须有指定一个形状，除非`validate_shape`设置为False。\n",
    "#                 trainable=True,  \n",
    "#                         # 如果`True`，则默认值也将变量添加到图形中集合`GraphKeys.TRAINABLE_VARIABLES`。\n",
    "#                         #这个集合用作“Optimizer”类使用的默认变量列表\n",
    "#                 collections=None,  \n",
    "#                         # 图表集合键的列表。新的变量被添加到这些集合。默认为`[GraphKeys.GLOBAL_VARIABLES]`。\n",
    "#                 validate_shape=True, \n",
    "#                         # 如果`False`，允许变量用初始化未知形状的值。如果“True”，默认的形状`initial_value`必须是已知的。\n",
    "#                  caching_device=None,  \n",
    "#                         # 可选设备字符串，描述变量的位置应该被缓存以供阅读。默认为变量的设备。如果不是“None”，则缓存在另一个设备上。\n",
    "#                         #典型的用途是缓存在使用变量 的Ops所在的设备上进行重复数据删除复制`Switch`和其他条件语句。\n",
    "#                  name='W',  \n",
    "#                         # 变量的可选名称。默认为“Variable”并获取自动去重（Variable_1,Variable_2....）。\n",
    "#                  variable_def=None,\n",
    "#                         # `VariableDef`协议缓冲区。如果不是“无”，则重新创建变量对象及其内容，引用变量的节点在图中，必须已经存在。\n",
    "#                         #图形没有改变。`variable_def`和其他参数是互斥的。\n",
    "#                 dtype=tf.float32,\n",
    "#                         # 如果设置，initial_value将被转换为给定的类型。如果`None'，数据类型将被保存\n",
    "#                         #（如果`initial_value`是一个张量），或者“convert_to_tensor”来决定。\n",
    "#                 expected_shape=None,  \n",
    "#                         # 张量的Shape。如果设置，initial_value需要符合这个形状。\n",
    "#                  import_scope=None\n",
    "#                         # 可选的字符串。名称范围添加到`Variable.`仅在从协议缓冲区初始化时使用。\n",
    "#                     ) \n",
    "# tf.Variable() 和tf.get_variable()\n",
    "# 1、使用tf.Variable时，如果检测到命名冲突，系统会自己处理。使用tf.get_variable()时，系统不会处理冲突，而会报错\n",
    "# 2、基于这两个函数的特性，当我们需要共享变量的时候，需要使用tf.get_variable()。在其他情况下，这两个的用法是一样的\n",
    "# 3、由于tf.Variable() 每次都在创建新对象，所有reuse=True 和它并没有什么关系。对于get_variable()，来说，\n",
    "# 如果已经创建的变量对象，就把那个对象返回，如果没有创建变量对象的话，就创建一个新的。\n",
    "\n",
    "#-----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 0 epoch,w is 3.800000,loss is 36.000000\n",
      "After 1 epoch,w is 2.840000,loss is 23.040001\n",
      "After 2 epoch,w is 2.072000,loss is 14.745600\n",
      "After 3 epoch,w is 1.457600,loss is 9.437184\n",
      "After 4 epoch,w is 0.966080,loss is 6.039798\n",
      "After 5 epoch,w is 0.572864,loss is 3.865470\n",
      "After 6 epoch,w is 0.258291,loss is 2.473901\n",
      "After 7 epoch,w is 0.006633,loss is 1.583297\n",
      "After 8 epoch,w is -0.194694,loss is 1.013310\n",
      "After 9 epoch,w is -0.355755,loss is 0.648518\n",
      "After 10 epoch,w is -0.484604,loss is 0.415052\n",
      "After 11 epoch,w is -0.587683,loss is 0.265633\n",
      "After 12 epoch,w is -0.670147,loss is 0.170005\n",
      "After 13 epoch,w is -0.736117,loss is 0.108803\n",
      "After 14 epoch,w is -0.788894,loss is 0.069634\n",
      "After 15 epoch,w is -0.831115,loss is 0.044566\n",
      "After 16 epoch,w is -0.864892,loss is 0.028522\n",
      "After 17 epoch,w is -0.891914,loss is 0.018254\n",
      "After 18 epoch,w is -0.913531,loss is 0.011683\n",
      "After 19 epoch,w is -0.930825,loss is 0.007477\n",
      "After 20 epoch,w is -0.944660,loss is 0.004785\n",
      "After 21 epoch,w is -0.955728,loss is 0.003063\n",
      "After 22 epoch,w is -0.964582,loss is 0.001960\n",
      "After 23 epoch,w is -0.971666,loss is 0.001254\n",
      "After 24 epoch,w is -0.977333,loss is 0.000803\n",
      "After 25 epoch,w is -0.981866,loss is 0.000514\n",
      "After 26 epoch,w is -0.985493,loss is 0.000329\n",
      "After 27 epoch,w is -0.988394,loss is 0.000210\n",
      "After 28 epoch,w is -0.990716,loss is 0.000135\n",
      "After 29 epoch,w is -0.992572,loss is 0.000086\n",
      "After 30 epoch,w is -0.994058,loss is 0.000055\n",
      "After 31 epoch,w is -0.995246,loss is 0.000035\n",
      "After 32 epoch,w is -0.996197,loss is 0.000023\n",
      "After 33 epoch,w is -0.996958,loss is 0.000014\n",
      "After 34 epoch,w is -0.997566,loss is 0.000009\n",
      "After 35 epoch,w is -0.998053,loss is 0.000006\n",
      "After 36 epoch,w is -0.998442,loss is 0.000004\n",
      "After 37 epoch,w is -0.998754,loss is 0.000002\n",
      "After 38 epoch,w is -0.999003,loss is 0.000002\n",
      "After 39 epoch,w is -0.999202,loss is 0.000001\n"
     ]
    }
   ],
   "source": [
    "w = tf.Variable(tf.constant(5, dtype=tf.float32))\n",
    "# lr : learn rate 学习率\n",
    "lr = 0.1\n",
    "# 循环次数\n",
    "epoch = 40\n",
    "\n",
    "for epoch in range(epoch):  # for epoch 定义顶层循环，表示对数据集循环epoch次，此例数据集数据仅有1个w,初始化时候constant赋值为5，循环40次迭代。\n",
    "    #tf.GradientTape(\n",
    "    #    persistent=False, watch_accessed_variables=True\n",
    "    #)\n",
    "    # 用于求导\n",
    "    # persistent=True 表示设置了持久，可以多次调用，默认是False，调用一次后，上下文中就没有了\n",
    "    # watch_accessed_variables=False 表示禁用自动跟踪被watch的变量，默认是True\n",
    "    with tf.GradientTape() as tape:  # with结构到grads框起了梯度的计算过程。\n",
    "        loss = tf.square(w + 1)\n",
    "    grads = tape.gradient(loss, w)  # .gradient函数告知谁对谁求导\n",
    "    # print(grads)\n",
    "    w.assign_sub(lr * grads)  # .assign_sub 对变量做自减 即：w -= lr*grads 即 w = w - lr*grads\n",
    "    print(\"After %s epoch,w is %f,loss is %f\" % (epoch, w.numpy(), loss))\n",
    "\n",
    "# lr初始值：0.2   请自改学习率  0.001  0.999 看收敛过程\n",
    "# 最终目的：找到 loss 最小 即 w = -1 的最优参数w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
